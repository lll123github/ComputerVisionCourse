{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\CondaEnvironment\\envs\\Visual\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "train_X shape: (60000, 28, 28)\n",
      "train_y shape: (60000,)\n",
      "test_X shape: (10000, 28, 28)\n",
      "test_y shape: (10000,)\n",
      "train_X[0]: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "train_y[0]: 5\n",
      "test_X[0]: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  84 185 159 151  60  36   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 222 254 254 254 254 241 198 198 198 198 198 198\n",
      "  198 198 170  52   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  67 114  72 114 163 227 254 225 254 254 254 250\n",
      "  229 254 254 140   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  17  66  14  67  67  67  59\n",
      "   21 236 254 106   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   83 253 209  18   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22\n",
      "  233 255  83   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129\n",
      "  254 238  44   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  59 249\n",
      "  254  62   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 133 254\n",
      "  187   5   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9 205 248\n",
      "   58   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 126 254 182\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  75 251 240  57\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  19 221 254 166   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3 203 254 219  35   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  38 254 254  77   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  31 224 254 115   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 133 254 254  52   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  61 242 254 254  52   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 121 254 254 219  40   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 121 254 207  18   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "test_y[0]: 7\n",
      "train_X[0].shape: (28, 28)\n",
      "train_y[0].shape: ()\n",
      "test_X[0].shape: (28, 28)\n",
      "test_y[0].shape: ()\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tuple'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_figures_X, train_y), (test_figures_X, test_y) = mnist.load_data(path='./datasets')\n",
    "print('train_X shape:', train_figures_X.shape)\n",
    "print('train_y shape:', train_y.shape)\n",
    "print('test_X shape:', test_figures_X.shape)\n",
    "print('test_y shape:', test_y.shape)\n",
    "print('train_X[0]:', train_figures_X[0])\n",
    "print('train_y[0]:', train_y[0])\n",
    "print('test_X[0]:', test_figures_X[0])\n",
    "print('test_y[0]:', test_y[0])\n",
    "print('train_X[0].shape:', train_figures_X[0].shape)\n",
    "print('train_y[0].shape:', train_y[0].shape)\n",
    "print('test_X[0].shape:', test_figures_X[0].shape)\n",
    "print('test_y[0].shape:', test_y[0].shape)\n",
    "print(type(train_figures_X[0]))\n",
    "print(type(train_y[0].shape))\n",
    "print(type(train_figures_X))\n",
    "print(type(train_y))\n",
    "image=train_figures_X[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HarrisCorner(image_array:np.ndarray)->int:\n",
    "    '''Harris角点特征提取'''\n",
    "    corners_mark = cv2.cornerHarris(image_array, blockSize=5,ksize=3,k=0.1)\n",
    "    corners_mark=cv2.erode(corners_mark,None)#腐蚀\n",
    "    corners_mark=cv2.dilate(corners_mark,None)#膨胀\n",
    "    # print(np.size(corner_image))\n",
    "    # print(np.size(image))\n",
    "\n",
    "\n",
    "    corners_image=image_array.copy()#注意这里一定要深拷贝\n",
    "    corners_image[corners_mark>0.005*corners_mark.max()]=255\n",
    "    '''角点坐标数组'''\n",
    "    # cv2.imshow('corners_image',corners_image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    count_array=np.zeros((image_array.shape[0],image_array.shape[1]))\n",
    "    '''计数使用的数组'''\n",
    "    count_array[corners_mark>0.005*corners_mark.max()]=1\n",
    "    # print(count_array[:,0])\n",
    "    corner_num:int=int(np.sum(count_array))\n",
    "    # print(corner_num)\n",
    "    return corner_num\n",
    "\n",
    "def getContours(image_array:np.ndarray)->tuple:\n",
    "    '''获取轮廓'''\n",
    "    # thresh, image_thresh = cv2.threshold(image_array, 127, 255, 0)\n",
    "    image_thresh = cv2.adaptiveThreshold(image_array,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,13,-10)\n",
    "    image_thresh=cv2.erode(image_thresh,None)#腐蚀\n",
    "    image_thresh=cv2.dilate(image_thresh,None)#膨胀\n",
    "    contours, hierarchy=cv2.findContours(image_thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "    contours:tuple\n",
    "    '''轮廓'''\n",
    "    # hierarchy输出各个轮廓的继承关系\n",
    "    # contours_image=cv2.drawContours(image_thresh,contours,-1,(127,255,127),2)\n",
    "    # cv2.imshow('contours_image',contours_image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "    return contours\n",
    "\n",
    "def getContoursNumPerimeterAndArea(image_array:np.ndarray)->tuple:\n",
    "    '''获取轮廓条数、长度和面积，其中返回的面积是轮廓圈出来的面积占总面积的比例'''\n",
    "    contours:tuple=getContours(image_array)\n",
    "    # print(len(contours))#轮廓数量\n",
    "    # print(contours)\n",
    "    #每一条轮廓是一个array。对于每一条轮廓由若干个点组成\n",
    "    contours_num:int=len(contours)\n",
    "\n",
    "    #计算所有轮廓周长\n",
    "    # contour_len=cv2.arcLength(contours[0],closed=True)\n",
    "    # print(contour_len)#第一条轮廓长\n",
    "    contours_len:float=0.0\n",
    "    '''轮廓总长度'''\n",
    "    for contour in contours:\n",
    "        contour_len=cv2.arcLength(contours[0],closed=True)\n",
    "        '''每个轮廓的长度'''\n",
    "        contours_len=contours_len+contour_len\n",
    "    # print(contours_len)#轮廓总长\n",
    "\n",
    "    #计算所有轮廓围成的面积\n",
    "    contours_area:float=0.0\n",
    "    for contour in contours:\n",
    "        contour_area=cv2.contourArea(contour,oriented=False)\n",
    "        contours_area=contours_area+contour_area\n",
    "    # print(contours_area)#轮廓围成的面积\n",
    "    contours_area_normalized:float=contours_area/(image_array.shape[0]*image_array.shape[1])\n",
    "    # print(contours_area_normalized)#归一化轮廓围成的面积\n",
    "    return (contours_num,contours_len,contours_area_normalized)\n",
    "\n",
    "def CannyEdgesPointsNum(image_array:np.ndarray)->int:\n",
    "    '''canny边缘点的个数'''\n",
    "    canny_low_threshold=30\n",
    "    canny_ratio=3\n",
    "    canny_kernel_size=3\n",
    "    canny_edges:np.ndarray=cv2.Canny(image_array,canny_low_threshold,canny_low_threshold*canny_ratio,apertureSize=canny_kernel_size)\n",
    "\n",
    "    # cv2.imshow('canny',canny_edges)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "    # np.set_printoptions(threshold=np.inf)\n",
    "    # print(canny_edges[0:10,:])\n",
    "    # print(type(canny_edges))\n",
    "    # print(canny_edges.shape)\n",
    "    #获得的数据不是有层次的，所以只能统计点的个数\n",
    "    canny_edges_points_num=int(np.sum(canny_edges)/255)\n",
    "    # print(canny_edges_points_num)\n",
    "    return canny_edges_points_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n",
      "(3, 102.42640614509583, 0.08418367346938775)\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "image_array=train_figures_X[0]\n",
    "print(HarrisCorner(image_array))\n",
    "print(getContoursNumPerimeterAndArea(image_array))\n",
    "print(CannyEdgesPointsNum(image_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(X:np.ndarray,y:np.ndarray)->pd.DataFrame:\n",
    "    columns_index:list=['HarrisCorner','ContoursNum','ContoursPerimeter','ContoursArea','CannyEdges','NumClassification']\n",
    "    features=pd.DataFrame(columns=columns_index,dtype='object')\n",
    "    for index in range(X.shape[0]):\n",
    "        image_array=X[index]\n",
    "        num_classification:int=y[index]\n",
    "        corner_num=HarrisCorner(image_array)\n",
    "        contours_num,contours_perimeter,contours_area=getContoursNumPerimeterAndArea(image_array)\n",
    "        contours_num:int\n",
    "        contours_perimeter:float\n",
    "        contours_area:float\n",
    "        canny_edges_points_num:int=CannyEdgesPointsNum(image_array)\n",
    "        features_line=pd.Series([corner_num,contours_num,contours_perimeter,contours_area,canny_edges_points_num,num_classification],index=columns_index,dtype='object')\n",
    "        # print(features_line)\n",
    "        # print(features_line)\n",
    "        features=pd.concat([features,features_line.to_frame().T],ignore_index=True,axis=0)\n",
    "    return features\n",
    "def getFeaturesX(features:pd.DataFrame)->np.ndarray:\n",
    "    features_X:np.ndarray=features.loc[:,'HarrisCorner':'CannyEdges'].values.astype('float')\n",
    "    features_X=np.insert(arr=features_X,obj=0,values=1,axis=1)\n",
    "    '''在最前面插入一列1，用于计算偏置'''\n",
    "    return features_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I行D+1列'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features:pd.DataFrame=getFeatures(train_figures_X,train_y)\n",
    "test_features:pd.DataFrame=getFeatures(test_figures_X,test_y)\n",
    "train_features_X:np.ndarray=getFeaturesX(train_features)\n",
    "'''I行D+1列'''\n",
    "test_features_X:np.ndarray=getFeaturesX(test_features)\n",
    "'''I行D+1列'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      HarrisCorner ContoursNum ContoursPerimeter ContoursArea CannyEdges  \\\n",
      "0              163           3        102.426406     0.084184         97   \n",
      "1               83           3         91.455844     0.076531        107   \n",
      "2              105           3              36.0      0.02551         84   \n",
      "3               56           1         52.627417     0.066327         60   \n",
      "4              113           2              24.0     0.023597         81   \n",
      "...            ...         ...               ...          ...        ...   \n",
      "59995          162           2              20.0     0.015306        109   \n",
      "59996          181           3         80.485281     0.079719         91   \n",
      "59997          113           2              20.0     0.026148         84   \n",
      "59998          110           1              10.0     0.007653         94   \n",
      "59999          146           2              16.0     0.010204         93   \n",
      "\n",
      "      NumClassification  \n",
      "0                     5  \n",
      "1                     0  \n",
      "2                     4  \n",
      "3                     1  \n",
      "4                     9  \n",
      "...                 ...  \n",
      "59995                 8  \n",
      "59996                 3  \n",
      "59997                 5  \n",
      "59998                 6  \n",
      "59999                 8  \n",
      "\n",
      "[60000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N:int=10\n",
    "'''类别数为10'''\n",
    "D:int=5\n",
    "'''维数为5，记得需要多加一维'''\n",
    "\n",
    "def softmax(x:np.ndarray)->np.ndarray:\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def logSoftmax(x:np.ndarray)->np.ndarray:\n",
    "    tmp=x-np.max(x)\n",
    "    return tmp-np.log(np.sum(np.exp(tmp)))\n",
    "\n",
    "def multiClassLogisticRegression(phy:np.ndarray,features_X:np.ndarray)->np.ndarray:\n",
    "    '''多分类逻辑回归'''\n",
    "    #前者是D+1行N列，后者是D+1行I列，两者相乘得到N行I列\n",
    "    return softmax(np.dot(phy.T,features_X.T))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=5.0927807565081824e+22，第0次迭代\n",
      "L=3.637265153289341e+23，第1次迭代\n",
      "L=9.918351323677708e+23，第2次迭代\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m phy\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdivide(phy,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m'''特征矩阵'''\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m phy:np\u001b[38;5;241m.\u001b[39mndarray\u001b[38;5;241m=\u001b[39mmultiClassLogisticRegressionTrain(train_features_X,train_y,phy,\u001b[38;5;241m1e-10\u001b[39m,\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     87\u001b[0m winsound\u001b[38;5;241m.\u001b[39mBeep(frequency, duration)\n",
      "Cell \u001b[1;32mIn [20], line 44\u001b[0m, in \u001b[0;36mmultiClassLogisticRegressionTrain\u001b[1;34m(features_X, y, phy, learning_rate, iteration_num)\u001b[0m\n\u001b[0;32m     42\u001b[0m g_n\u001b[38;5;241m=\u001b[39mg_n\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(x_i\u001b[38;5;241m.\u001b[39mreshape(D\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m),((y_i\u001b[38;5;241m-\u001b[39mw_i_vector)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,N)))\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m'''梯度'''\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m H_n\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m kernel\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(x_i\u001b[38;5;241m.\u001b[39mreshape(D\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m),x_i\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,D\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import winsound\n",
    "duration = 1000 # 持续时间/ms\n",
    "frequency = 500 # 频率/Hz\n",
    "\n",
    "\n",
    "\n",
    "def multiClassLogisticRegressionTrain(features_X:np.ndarray,y:np.ndarray,phy:np.ndarray,learning_rate:float,iteration_num:int)->np.ndarray:\n",
    "    '''多分类逻辑回归训练'''\n",
    "    I:int=features_X.shape[0]\n",
    "    '''训练样本数'''\n",
    "    g_n:np.ndarray=np.zeros((D+1,N))\n",
    "    '''梯度'''\n",
    "    H:np.ndarray=np.zeros(shape=(N,N,D+1,D+1))\n",
    "    '''Hessian矩阵'''\n",
    "    for iteration_index in range(iteration_num):\n",
    "        #实现算法9.8\n",
    "        L=0\n",
    "        '''目标函数'''\n",
    "\n",
    "        #for 循环内部是copilot帮忙写的，我检查了一遍，没有什么问题，非常巧妙\n",
    "        for i in range(I):\n",
    "            # print(f\"---------------------------第{i}个样本—------------------------\")\n",
    "            # print(f\"前梯度：{g_n}\")\n",
    "            # # print(f\"前Hessian矩阵：{H}\")\n",
    "            # print(f\"前phy：{phy}\")\n",
    "            x_i:np.ndarray=features_X[i]\n",
    "            '''第i个样本'''\n",
    "            y_i=softmax(np.dot(phy.T,x_i))\n",
    "            w_i:int=y[i]\n",
    "            '''第i个样本的类别'''\n",
    "            w_i_vector=np.zeros(N)\n",
    "            '''第i个样本的类别向量，N行1列'''\n",
    "            w_i_vector[w_i]=1\n",
    "            log_y_i=logSoftmax(np.dot(phy.T,x_i))\n",
    "            delta_L=-np.dot(w_i_vector,log_y_i)\n",
    "            L=L+delta_L\n",
    "            # print(f\"softmax前：{np.dot(phy.T,x_i)}\")\n",
    "            # print(f\"第{i}个样本，预测类别向量：{np.argmax(y_i)}，真实类别：{w_i}，损失：{delta_L}\")\n",
    "            # print(f\"logSoftmax(np.dot(phy.T,x_i))：{log_y_i}\")\n",
    "            '''目标函数'''\n",
    "            #后者是1行N列，前者是D+1行1列，两者相乘得到D+1行N列\n",
    "            g_n=g_n+np.dot(x_i.reshape(D+1,1),((y_i-w_i_vector).reshape(1,N)))\n",
    "            '''梯度'''\n",
    "            H_n=np.zeros(shape=(N,N,D+1,D+1))\n",
    "            kernel=np.dot(x_i.reshape(D+1,1),x_i.reshape(1,D+1))\n",
    "            for n in range(N):\n",
    "                for m in range(N):\n",
    "                    H_n[m,n]=y_i[m]*(np.eye(N)[m,n]-y_i[n])*kernel\n",
    "                    \n",
    "            H=H+H_n\n",
    "            for n in range(N):\n",
    "                for m in range(N):\n",
    "                    # print(H[m,n].shape)\n",
    "                    # print(H[m,n])\n",
    "                    # print(learning_rate*np.dot(np.linalg.inv(H[m,n]),g_n))\n",
    "                    #前者是D+1行D+1列，后者是D+1行N列，两者相乘得到D+1行N列\n",
    "                    try:\n",
    "                        tmp=np.dot(np.linalg.inv(H[m,n]),g_n)\n",
    "                        phy=phy-learning_rate*tmp#TODO 这个地方有问题\n",
    "                    except Exception as e:\n",
    "                        print(f'Exception:{e}')\n",
    "                        print(f'H_n[m,n]:{H_n[m,n]}')\n",
    "                        print(f'H[m,n]:{H[m,n]}')\n",
    "                        print(f'y_i:{y_i}')\n",
    "                        print(f'm:{m}')\n",
    "                        print(f'n:{n}')\n",
    "                        print(f'y_i[m]:{y_i[m]}')\n",
    "                        print(f'y_i[n]:{y_i[n]}')\n",
    "                        winsound.Beep(frequency, duration)\n",
    "                        return\n",
    "            # print(f\"x_i={x_i}\")\n",
    "            # print(f\"y_i={y_i}\")\n",
    "            # print(f\"w_i={w_i}\")\n",
    "            # print(f\"后梯度：{g_n}\")\n",
    "            # # print(f\"后Hessian矩阵：{H}\")\n",
    "            # print(f\"后phy：{phy}\")\n",
    "            # print(f\"phy的形状：{phy.shape}\")\n",
    "            # print(f\"H[m,n].shape:{H[m,n].shape}\")\n",
    "            # print(f\"g_n.shape:{g_n.shape}\")\n",
    "        print(f'L={L}，第{iteration_index}次迭代')\n",
    "    return phy\n",
    "\n",
    "phy=np.ones((D+1,N))\n",
    "phy=np.divide(phy,2)\n",
    "'''特征矩阵'''\n",
    "phy:np.ndarray=multiClassLogisticRegressionTrain(train_features_X,train_y,phy,1e-10,100)\n",
    "winsound.Beep(frequency, duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均差值：0.902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP_Lin-uCong\\AppData\\Local\\Temp\\ipykernel_48060\\707241233.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  return e_x / e_x.sum(axis=0)\n"
     ]
    }
   ],
   "source": [
    "test_y_predict:np.ndarray=multiClassLogisticRegression(phy,test_features_X)\n",
    "\n",
    "#统计\n",
    "test_y_hut=np.argmax(test_y_predict,axis=0)\n",
    "'''预测类别'''\n",
    "test_y_telda=test_y-test_y_hut\n",
    "'''差值'''\n",
    "test_y_difference=np.zeros(test_y_telda.shape[0])\n",
    "test_y_difference[np.nonzero(test_y_telda)]=1\n",
    "'''差值不为0的地方置1'''\n",
    "sum_difference=np.sum(test_y_difference)\n",
    "'''差值不为0的个数'''\n",
    "average_difference=sum_difference/test_y_telda.shape[0]\n",
    "'''平均差值'''\n",
    "print(f\"平均差值：{average_difference}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Visual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
